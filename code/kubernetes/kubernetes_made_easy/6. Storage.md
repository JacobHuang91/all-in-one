At some point, your apps requires storage where the data is stored and accessed

- How does this storage volumes are handled inside the Kubernetes cluster?
- How can data persist beyond pod life?
- How do these containers share data among them?

Volumes

- Pods are ephemeral and stateless
- Volumes bring persistence to pods
- Adv of Kubernetes volumes vs. Docker volumes
    - Container(s) has access to volume
    - Volumes are associated with Lifecycle of pod
    - Supports many types of volumes

# Volumes types

![img](storage-1.png)

![img](storage-2.png)

# emptyDir

- Creates empty directory first created when a Pod is assigned to a Node,
- Stays as long as pod is running
- Once pod is removed from a node, emptyDir is deleted forever

Use cases:

- Temporary space
- share data with containers in a pod

![img](storage-3.png)
![img](storage-4.png)
![img](storage-5.png)
![img](storage-6.png)

test-ed.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-ed
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/test-webserver
      volumeMounts:
        - name: cache-volume
          mountPath: /cache
  volumes:
    - name: cache-volume
      emptyDir: { }
```

```bash
kubectl create -f test-ed.yaml
kubectl get po -o wide
# df disk filesystem
# it is used to get a full summary of available and used disk space usage of the file system on Linux system
kubectl exec test-ed df /test-mnt 
kubectl describe pod test-ed

kubectl delete po test-ed
```

# hostPath

- mounts a file or directory from the host node’s filesystem into your Pod
- Remains even after the pod is terminated
- Similar to docker volume
- Use cautiously when required
- Host issues might cause problem to hostPath

![img](storage-7.png)

![img](storage-8.png)

双向同步

![img](storage-9.png)

![img](storage-10.png)

删除pod之后，数据仍然在
![img](storage-11.png)

nginx-hostpath.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-hostpath
spec:
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
        - mountPath: /test-mnt
          name: test-vol
  volumes:
    - name: test-vol
      hostPath:
        path: /test-vol
```

```bash
kubectl create -f nginx-hostpath.yaml
kubectl get po
kubectl exec nginx-hostpath df /test-mnt

#From HOST
cd /test-vol
echo "From Host" > from-host.txt
cat from-host.txt
#From POD
kubectl exec nginx-hostpath cat /test-mnt/from-host.txt


#From POD
kubectl exec nginx-hostpath -it -- /bin/sh
cd /test-mnt
echo "From Pod" > from-pod.txt
cat from-pod.txt
#From Host
cd /test-vol
ls
cat from-pod.txt

kubectl delete po nginx-hostpath
kubectl get po
ls /test-vol
```

# gcePersistentDisk

> Imagine, you are using google cloud instances as Kubernetes master and worker nodes

Your requirement is to have persistent mount point inside your pod to store application data

What is the appropriate volume type?

- Volume mounts a Google Compute Engine (GCE) Persistent Disk into Pod
- Volume data is persisted pods termination
- Read-Write only on one node and Read-Write on many nodes

Restrictions:

- You must create a PD using gcloud or the GCE API or UI before you can use it
- the nodes on which Pods are running must be GCE VMs
- those VMs need to be in the same GCE project and zone as the PD

![img](storage-12.png)
![img](storage-13.png)
![img](storage-14.png)
![img](storage-15.png)
![img](storage-16.png)
![img](storage-17.png)
![img](storage-18.png)
![img](storage-19.png)

```bash
#1. Create Disk on Google Cloud

# From Dashboard
# OR command
gcloud compute disks create --size=10GB --zone=us-central1-a my-data-disk
gcloud compute disks list
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gce-pd
spec:
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
        - mountPath: /test-pd
          name: test-volume
  volumes:
    - name: test-volume
      gcePersistentDisk:
        pdName: my-data-disk
        fsType: ext4
```

```bash
kubectl create -f gcepd.yaml
kubectl get po -o wide
kubectl describe po gce-pd

kubectl exec gce-pd -it -- /bin/sh
df
echo "Testing - 1" > /test-pd/test1.html
exit

kubectl delete -f gce-pd.yaml

kubectl create -f gce-pd.yaml
kubectl get po -o wide

kubectl exec gce-pd -it /bin/sh
ls /test-pd/
cat /test-pd/test1.html

kubectl delete -f gce-pd.yaml
kubectl get pods
```

# Persistent Volumes & Persistent Volume Claims (PV & PVC)

> Your company’s infrastructure team provides different storage solutions
> such as Block, NAS, Object storage, Google Cloud disks, AWS disks and more.

> There need to be a consistent way to deal with all these storage types
> What is Kubernetes solution for it?


![img](storage-20.png)

![img](storage-21.png)

- 持久卷（PersistentVolume，PV）是集群中的一块存储， 可以由管理员事先供应，或者 使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。 PV 持久卷和普通的 Volume
  一样，也是使用 卷插件来实现的， 只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 此 API 对象中记述了存储的实现细节， 无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。
- 持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。 概念上与 Pod 类似。 Pod 会耗用节点资源， 而 PVC 申领会耗用 PV 资源。 Pod 可以请求特定数量的资源（CPU 和内存）；
  同样 PVC 申领也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载，参见访问模式）

![img](storage-22.png)
![img](storage-23.png)
![img](storage-24.png)
![img](storage-25.png)

# Static volume provisioning

![img](storage-26.png)

- admin 创建pv
- developer 用pvc申请拿到pv
- 如果容量不够，只能等admin创建更大的pv

![img](storage-27.png)
![img](storage-28.png)
![img](storage-29.png)
![img](storage-30.png)
![img](storage-31.png)
![img](storage-32.png)
![img](storage-33.png)
![img](storage-34.png)
![img](storage-35.png)
![img](storage-36.png)
![img](storage-37.png)
![img](storage-38.png)
![img](storage-39.png)

# Dynamic volume provisioning

- 创建storage class

![img](storage-40.png)

![img](storage-41.png)
![img](storage-42.png)

可以选择要多少内存

![img](storage-43.png)

![img](storage-44.png)
![img](storage-45.png)
![img](storage-46.png)
![img](storage-47.png)
![img](storage-48.png)
![img](storage-49.png)
![img](storage-50.png)
![img](storage-51.png)

```yaml
# sc.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
```

```bash
kubectl get sc
kubectl create -f sc.yaml
kubectl get sc
kubectl describe storageclass fast
```

```yaml
# pvc1.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-disk-claim-1
spec:
  resources:
    requests:
      storage: 30Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
```

```yaml
# pvc2.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: my-disk-claim-2
spec:
  resources:
    requests:
      storage: 40Gi
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
```

```bash
kubectl create -f pvc1.yaml
kubectl create -f pvc2.yaml
kubectl get pvc
```

```yaml
# nginx-pv.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
    - name: test-container
      image: nginx
      volumeMounts:
        - mountPath: /test-pd
          name: test-volume
  volumes:
    - name: test-volume
      persistentVolumeClaim:
        claimName: my-disk-claim-1
```

```bash
kubectl create -f nginx-pv.yaml
kubectl get po -o wide

kubectl exec pv-pod -it -- /bin/sh
df /test-pd
cd /test-pd 
echo "From test-1" > test1.txt            
exit

# delete
kubectl delete -f nginx-pv.yaml
kubectl get po -o wide

# recreate
kubectl create -f nginx-pv.yaml
kubectl get po -o wide

kubectl exec pv-pod df /test-pd
kubectl exec pv-pod ls /test-pd/
kubectl exec pv-pod cat /test-pd/test1.txt

kubectl delete -f nginx-pv.yaml
kubectl delete -f pvc1.yaml
kubectl delete -f pvc2.yaml
kubectl delete -f sc.yaml
kubectl get pods
kubectl get pvc
kubectl get sc
```